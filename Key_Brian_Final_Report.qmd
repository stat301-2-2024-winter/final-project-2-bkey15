---
title: "Final Project: A Model to Estimate HR Scores (2020-)"
subtitle: |
  | Data Science 2 with R (STAT 301-2)
author: "Brian Key"
date: today

format:
  html:
    toc: true
    embed-resources: true
    theme:
      dark: darkly

number-sections: true

execute:
  echo: false
  warning: false

from: markdown+emoji 
reference-location: margin
citation-location: margin
---

```{r, echo = FALSE}
#| label: load-pkgs
#| code-fold: false

library(tidyverse)
library(DT)
library(here)
```

::: {.callout-tip icon=false}

## Github Repo Link

[Final Project Github Repo (BDK)](https://github.com/stat301-2-2024-winter/final-project-2-bkey15)

:::
## Introduction

### Prediction Problem

Two of my three dissertation chapters rely on the use of a dependent variable measuring human rights respect. Hitherto, I have depended on Fariss's Human Rights (HR) Scores,^[For the original article wherein these scores were introduced, see [Fariss, 2014](http://cfariss.com/documents/Fariss2014APSR.pdf).] but he has not updated his dataset since 2020.^[See his [Dataverse page](https://dataverse.harvard.edu/dataverse/HumanRightsScores) for evidence.] This poses a problem insofar as it excludes the most recent country-year observations from my universe of cases entirely.^[That is, it is impossible to evaluate the relationship(s) between independent and dependent variables for cases where the latter is systematically missing.] In my [previous final project](https://github.com/stat301-1-2023-fall/final-project-1-bkey15), and by way of exploratory data analyses (EDAs), I identified a "raw" form of the physical violence index (PVI)---a simple average of the freedoms from political killings and torture indicators featured in the [Varieties of Democracy (V-Dem) dataset](https://v-dem.net/)---as a potential substitute for HR scores. However, for this project, I aim to predict outright the missing data in HR scores whilst more formally appraising the suitability of the raw PVI as a simple substitute for HR scores.

### Key Findings

After implementing and assessing a diversity of methods to predict HR scores, as summarized in this report, I render the following conclusions:

1. There exist more accurate ways to approximate HR scores than to use the raw PVI in lieu of it (see @sec-base-fits).
2. Of these, the best-performing is a KNN-regression model (see @sec-tune-fits).
3. Time-series models exhibit even lower mean prediction errors, yet evidence suggests they may feature greater variance vis-à-vis the KNN model (see @sec-ts).

### Overview of Report

The remainder of my report is organized as follows:

1.  An overview of HR scores and V-Dem, including an EDA of the former (@sec-dat-ov).
2.  A description of initial data preprocessing (@sec-preproc).
3.  A summary of the recipes used (@sec-rec).
4.  The results of the baseline fits (@sec-base-fits).
5.  The results of the tuned fits (@sec-tune-fits).
6.  The final fit's predictions and an analysis thereof (@sec-final-fit).
7.  A discussion of time-series predictions and how they compare to those of the final fit (@sec-ts)
8.  Appendices (@sec-appen)

## Data Overview {#sec-dat-ov}

### HR Scores

The HR Scores dataset, which features the HR scores variable,^[This variable is denominated `theta_mean` in the original dataset. For the remainder of the report, I use "HR scores" to refer to this specific variable, not the dataset from which it originates.] consists of 11275 country-year observations spanning the period 1946-2019. The HR scores variable lacks missingness, with every observation possessing an HR score.

#### EDA of HR Scores

The distribution of HR scores may be visualized as follows:

![Distribution of HR Scores (Density & Box Plots)](plots/hr_score_OG_dist_plots.png){#fig-hr-dist}

@fig-hr-dist shows that the variable isn't distributed in a perfectly symmetrical manner, exhibiting a slight right-skew and a degree of bimodality. Nevertheless, these features seem relatively minor, and most methods available to correct them do not apply given the nature of the variable.^[Namely, square-root, log, and Box-Cox transformations only work for positive variables, but approximately half of HR Scores' values are negative.] I also attempted a Yeo-Johnson transformation of HR scores, but it failed to adequately rectify the problems of skewness and bimodality.^[For more, see @fig-yj] In view of these considerations, I proceed by keeping HR scores unaltered.

### V-Dem

The 2024 version of V-Dem (V14) fully consists of 4607 variables and 27734 country-year observations spanning the period 1789-2023. 12185 of these observations cover the years 1946-2019, whereas 716 of them cover 2020-2023. Overall, the dataset features an appreciable degree of missingness, with approximately 26.36\% of values in the 1946-2019 period being absent.^[This figure is calculated from all non-ID variables. See `scripts/1b_data_quality_check.R` for the computation.] However, rates of missingness are vastly lower for the five "high-level" democracy indices (approximately 0.30\%) and the twenty-one "mid-level" components of said indices (approximately 0.59\% ).^[See `scripts/1b_data_quality_check.R` again for the relevant computations.] These missingness rates are reduced even further following pre-recipe preprocessing (see @sec-preproc).

## Pre-Recipe Preprocessing {#sec-preproc}

Prior to writing my recipes, I complete the following noteworthy preprocessing steps:

-   Merge V-Dem and HR Scores.^[My actual code also merges these datasets with the Political Terror Scale (PTS) dataset. At the outset, I desired to include variables from the latter in my models, as they were explicitly noted as inputs for the original HR Scores; however, they featured widespread missingness, to the extent that imputing their values proved to be computationally infeasible.]
-   For V-Dem, recode Czechoslovakia's pre-Velvet Divorce (1992) observations as those of the Czech Republic.^[I.e., recode `cowcode == 315` (Czechoslovakia) to `cowcode == 316` (Czech Republic).] Doing so rectifies widespread missingness in this case. I deemed this justifiable on two counts:
    1. Fariss assigns the Czech Republic COW code to Czechoslovakia.
    2. V-Dem denominates both Czechoslovakia and the Czech Republic "Czechia"; there is no overlap in naming conventions or otherwise with Slovakia, which doesn't appear in the dataset until 1993.
-   Given the wholesale absence of predictors and thus the virtual impossibility of prediction-making, remove country-years that appear in HR scores but not in V-Dem. These cases appear in @tbl-vdem-miss, below, where the value on "N Missing" is positive:

```{r, echo = FALSE}
#| label: tbl-vdem-miss
#| tbl-cap: "N Missing from V-Dem by Country"
load(here("data/preprocessed/dattbl_vdem_miss.rda"))
dattbl_vdem_miss
```

-   These observations mainly involve:
    1. Post-Soviet states (e.g., Armenia, Azerbaijan, etc.) and post-WWII states (e.g., West and East Germany).
    2. European microstates (e.g., Liechtenstein and Monaco) and small island states (e.g., Samoa, St. Vincent and the Grenadines, etc.).
-   The former is more tolerable, for they stem from minor discrepancies in coding start/end dates and only pertain to a handful of years. The latter invites more reason for pause; indeed, to delete these observations is effectively to remove the world's smallest countries from the ambit of my research entirely. Nevertheless, it doesn't make sense, in my estimation, to retain them when there exist no data in V-Dem whatsoever---including the most basic of figures, such as population, GDP, etc.---on which even somewhat-credible imputations might be based.
-   Select as predictors the variables I identified in my previous final project as being related in theory to HR scores, as well as the entirety of V-Dem's high-level and medium-level indices. I do so primarily to minimize my computational workload, but also under the intuition that including variables beyond this set would yield diminishing returns with respect to accuracy.^[That is, including more variables would bring fewer improvements to predictive accuracy at the expense of computational speed.]

Upon considering and completing these steps, I move forward with a preprocessed dataset of 49 variables and 10524 observations.^[My saved preprocessed dataset actually contains 52 variables, but three of these are PTS metrics that are removed at the beginning of recipe preprocessing.] The plot below demonstrates that the preprocessing steps involving HR scores does not significantly alter the distribution thereof:^[Compare with @fig-hr-dist. Admittedly, the distribution might be marginally more right-skewed; this likely owes itself to the removal of the European microstates, whose human rights levels are generally very high.]

![Distribution of HR Scores, Preprocessed (Density & Box Plots)](plots/hr_score_dist_plots.png){#fig-hr-dist-preproc}

The preprocessed dataset also exhibits an exceedingly low degree of missingness relative to that of V-Dem overall (see @sec-dat-ov), with only about about 0.19% of values from non-ID variables being missing. @fig-preproc-miss, below, summarizes this missingness by predictor:

![Number of Missing Values by Predictor, Preprocessed Dataset](plots/preproc_miss_plot.png){#fig-preproc-miss}

Ultimately, in virtue of the minimal extent of missingness, imputations are unlikely to prove computationally demanding or to appreciably qualify the credibility of predictions generated therefrom.

### Data Splitting Specifications

Upon completion of these preprocessing steps, I split the data with proportions of 75-25 training/testing, 90-10 analysis/assessment. The cross-validation split is repeated five times, resulting in a total of 50 folds.

## Recipes {#sec-rec}

My null (`null`) and basic baseline (`lm`) models (see @sec-base-fits) draw on three recipes, each distinguished by the number of neighbors set for KNN-imputation: 5, 10, and 20, respectively.^[I considered bagged-tree imputation as well, but it proved computationally infeasible.] I establish and test these recipe variants, at this stage, to appraise the extent to which changes in the number of neighbors set for imputation effectuate changes in model performance.

Aside from KNN-imputation, the main preprocessing components shared by each recipe-group are as follows:

-   Transforming country ID (`cowcode`) and year to a dummy variable.
-   Log-transforming population, GDP, and GDP-per-capita.^[These variables are widely-known as right-skewed, and they are indeed so in the V-Dem dataset. For evidence that I have verified this, see `scripts/appendices/0_appendices.R`.]
-   Normalizing all numeric predictors.
-   Removing the year dummies subsequent to KNN-imputation.

This final step is important. As a factor, year can continue to be used as an imputation predictor for V-Dem observations, because there will almost certainly be V-Dem data for future years;^[The 2024 V-Dem Dataset, which will provide data up to 2023, is scheduled to be released on 07 March 2024.] yet it cannot be used to predict HR Scores, for that variable ends in 2019.^[Put differently, because year is a factor, there is no way to estimate the relationship between "2023," "2024," etc. and HR Scores when there exists wholesale missingness of HR Scores for those years.]

In aggregate, the preprocessing steps results in a training set of 226 variables: 1 outcome, 2 ID variables,^[`country_name` and `cow_year`] and 223 predictors---179 of which are country dummies.

Also included as a baseline in @sec-base-fits is a fourth recipe (`akt_lm`), which simply avails itself of my unscaled Political Violence Index (PVI), created in my previous final project, as the sole predictor of HR Scores. I do so in order to test the appropriateness of merely substituting it for HR Scores.

The ridge, lasso, and elastic-net models seen in @sec-tune-fits use the same recipe as that underpinning the 5-neighbor baselines.^[See @sec-base-fits, below, for a discussion as to why I proceed with the 5-neighbor recipe for the tuning fits.] The KNN, random forest, and boosted tree models---also appearing in @sec-tune-fits---rely on a similar recipe, the only difference being the inclusion of one-hot encoding for the country-ID dummy variables.^[This means that the total number of predictors utilized by these models is 224---one more than that of the others.]

To summarize, the recipes I deploy and their apportionment to the models can be organized as follows:

1.    Main:
  -   5 Neighbors: basic baseline, null, ridge, lasso, elastic net
  -   10 Neighbors: basic baseline, null
  -   20 Neighbors: basic baseline, null
2.    Tree:
  -   5 Neighbors: KNN, random forest, boosted tree
3.    Unscaled PVI

These amount to five recipes in total: three "main," one "tree," and one additional baseline (unscaled PVI).

## Baseline Fits {#sec-base-fits}

@tbl-rmses-base, below, gives the performance of my baseline fits on the cross-validation folds as measured by mean RMSE:

```{r, echo = FALSE}
#| label: tbl-rmses-base
#| tbl-cap: "Comparison of Mean RMSEs by Model"
load(here("data/results/fits_cv/baselines/rmse_kbl_base_fits.rda"))
rmse_kbl_base_fits
```

As we can see, the null models perform well-worse than the basic baseline models; the unscaled PVI baseline splits the difference between the two.

There are two important takeaways at this juncture, in my view. First, the basic baseline outperforms the unscaled PVI baseline, meaning we can proceed knowing that there exist better ways to estimate HR Scores than to use the unscaled PVI as a simple substitute for HR Scores. Second, changes in the number of neighbors used for KNN-imputation seem to have little effect on model performance. This is good insofar as it is no longer a concern of ours; we will proceed with the `step_impute_knn()` default of `neighbors = 5` for all remaining workflows, though we theoretically could have selected a larger (or smaller) number with marginal impact on our findings.

## Tuned Fits {#sec-tune-fits}

I subsequently tune the following hyperparameters for six models on the cross-validation folds:

1.    Ridge: `penalty`
2.    Lasso: `penalty`
3.    Elastic Net: `penalty` and `mixture`
4.    KNN: `neighbors`
5.    Random Forest: `mtry` and `min_n`
6.    Boosted Tree: `mtry`, `min_n`, and `learn_rate`

For the creation of the respective tuning grids, the first four use `levels = 10`, whereas the latter two use `levels = 5`. The random forest model further uses `trees = 1000` and an `mtry` range set to `c(1, 15)`, whereas the boosted tree model uses the same `mtry` range and a `learn_rate` range set to `c(-3, -0.2)`.

The processing times for these models, with parallel processing across eight cores where possible, were approximately as follows:^[I simply timed these with my computer's clock.]

1.    Ridge: 5 minutes
2.    Lasso: 4 minutes
3.    Elastic Net: 9 minutes
4.    KNN: 12 minutes
5.    Random Forest: 92 minutes
6.    Boosted Tree: 65 minutes

Below is @tbl-rmses-tune1, which gives the best mean RMSE of each tuned fit:

```{r, echo = FALSE}
#| label: tbl-rmses-tune1
#| tbl-cap: "Comparison of Best Mean RMSEs by Model"
load(here("data/results/fits_cv/tuned/rmse_kbl_best_tuned1.rda"))
rmse_kbl_best_tuned1
```

The optimal tuning parameters for these models as assessed by mean RMSE are, respectively:

-   KNN: `neighbors = 2`
-   Random Forest: `mtry = 14`, `min_n = 2`
-   Boosted Tree: `mtry = 14`, `min_n = 2`, `learn_rate = 0.631`
-   Elastic Net: `penalty = 1e-10`, `mixture = 0.683`
-   Lasso: `penalty = 1e-10`
-   Ridge: `penalty = 1e-10`

Below is also a plot depicting the confidence intervals for each estimate:

![](plots/ci_plot_best_tuned1.png)

```{r, echo = FALSE}
#| label: tbl-rmses-tune2
#| tbl-cap: "Comparison of Best Mean RMSEs by Model"
load(here("data/results/fits_cv/tuned/rmse_kbl_best_tuned2.rda"))
rmse_kbl_best_tuned2
```

![](plots/ci_plot_best_tuned2.png)

@tbl-rmses-tune1 and the above plot evince that, while the best random-forest model possesses the lowest standard error, it is the best KNN model that has the lowest mean RMSE and confidence interval thereof. As such, we proceed with our final fit by selecting the best KNN model.

## Final Fit {#sec-final-fit}

@tbl-metrics-final, below, gives the performance metrics for the final fit applied to the testing set:

```{r}
#| label: tbl-metrics-final
#| tbl-cap: "Performance Metrics from Final Fit"
load(here("data/results/final/final_perform_stats_kbl.rda"))
final_perform_stats_kbl
```

The RMSE is even lower when compared to that of the cross-validation folds (@tbl-rmses-tune2). The MAE is about half has large as the RMSE; and at approximately 0.975, the $R^{2}$ statistic is exceedingly high. (The MAPE is about 51.2\%, but this is a small figure in absolute terms, the preponderance of HR scores being clustered around 0.)

The RMSE of approximately 0.227 represents an exceedingly marginal difference. For the full set of HR Scores, the minimum is approximately `r round(-3.459534, digits = 2)`, while the maximum is approximately `r round(5.336182, digits = 2)`, for a range of approximately `r round(8.795716, digits = 2)`; the RMSE's range,^[Computed (approximately) by $0.227*2$.] then, represents only about `r round(((0.2271517*2)/8.795716)*100, digits = 2)`\% of the full range. Recalling that HR Scores are themselves estimates with a mean standard deviation of approximately `r round(0.3338768, digits = 2)` lends further credibility to the accuracy of our model.^[For the code computing this figure, see `scripts/appendices/0_appendices.R`.] Qualitatively, a score difference of 0.227 does not seem to mean much either. Indeed, the examples of countries that experienced such a score change did not witness, to my knowledge, any appreciable changes in their underlying political milieus.^[For these examples, see `scripts/appendices/0_appendices.R`.]

To round off this discussion is the below scatterplot, which depicts the relationship between our predictions and actual values in the testing set, illustrating the tight fit---and hence high degree of accuracy---of our final model:

![](plots/scatt_plot_final_predicts.png)

## Time-Series Predictions {#sec-ts}

### Remaining Considerations

At this stage, I feel as though my final project is near to completion. Aside from the issues I've already identified as meriting potential feedback---namely, Yeo-Johnson transformation of the outcome (@sec-dat-ov) and the removal of certain countries from the dataset (@sec-preproc)---I welcome advice as to how to think about, or tackle if necessary, the issue of multicollinearity.

Needless to say, four of my five recipes are effectively "kitchen sink" in kind (the outlier being the unscaled PVI recipe). The V-Dem variables capture interrelated phenomena (e.g., elections and civil liberties) and occasionally comprise additive measures as well as their inputs (e.g., PVI and its components, the political killings and torture metrics). They also contain variants of the same variable set to different scales (e.g., the three-point, four-point, and five-point versions of the PVI). A particularly concerning example of this, to me, is the inclusion of both the unscaled PVI and original (unscaled) PVI, the latter simply being the former set to a 0-to-1 scale. The two are hence tightly related, namely through a logistic function, as demonstrated in the plot below:^[My work in this section can be found in `scripts/appendices/`.]

![](plots/scatt_plot_pvi.png)

I opted to include all such variants, for I didn't want to presuppose that one would be "better" in predicting HR Scores than the other(s). I'm also of the understanding that multicollinearity may be less of a concern for prediction problems such as ours, where the primary objective is simply to produce the most accurate estimates possible---in contrast to inference problems, where robust coefficients and p-values are the gold standard. Nonetheless, I remain worried about the impact of multicollinearity on the strength of my predictions.

To this end, I created and briefly tested two sets of new recipes from my original set of "main" recipes (@sec-rec), both of which are multicollinearity averse. The first is less averse, simply removing the scaled PVI^[`v2x_clphy`] as a predictor from each main recipe.^[I do, however, allow the scaled PVI to be present for the KNN-imputation step. This is also true for all other variable-variants in the more-averse set of recipes.] The second is more averse, removing not only said variable, but also all other variable-variants as predictors.^[These variables are, namely, the PVI Ordinal (`e_v2x_clphy *_3C, *_4C, *_5C`), the Equality before the Law and Individual Liberty Index Ordinal (`e_v2xcl_rol *_3C, *_4C, *_5C`), and the Civil Liberties Index Ordinal (`e_v2x_civlib *_3C, *_4C, *_5C`).]

For now, I tested these recipes on the baseline models, exclusively. Therefore, the recipes I deployed and their apportionment to the models can be organized as follows:

1.    Least Averse:
  -   5 Neighbors: basic baseline, null
  -   10 Neighbors: basic baseline, null
  -   20 Neighbors: basic baseline, null
2.    Most Averse:
  -   5 Neighbors: basic baseline, null
  -   10 Neighbors: basic baseline, null
  -   20 Neighbors: basic baseline, null

These amount to six recipes in total: three that are less multicollinearity averse, and three that are more multicollinearity averse.

The performance of these fits on the cross-validation folds, as measured by mean RMSE, are given in @tbl-rmses-base-la and @tbl-rmses-base-ma, below:

```{r, echo = FALSE}
#| label: tbl-rmses-base-la
#| tbl-cap: "Least Averse - Comparison of Mean RMSEs by Model"
load(here("data/results/fits_cv/baselines/appendices/rmse_kbl_base_fits_la.rda"))
rmse_kbl_base_fits_la
```

```{r, echo = FALSE}
#| label: tbl-rmses-base-ma
#| tbl-cap: "Most Averse - Comparison of Mean RMSEs by Model"
load(here("data/results/fits_cv/baselines/appendices/rmse_kbl_base_fits_ma.rda"))
rmse_kbl_base_fits_ma
```

The basic baseline (`*_lm`) results seemingly signify a deterioration in quality: vis-à-vis the metrics seen in @tbl-rmses-base, the models perform worse, with gradual yet consistent increases in both mean RMSE and standard error as they become more multicollinearity averse.

If predictive accuracy is the chief objective, then these results suggest that I should not, in fact, proceed with the multicollinearity-averse recipes---irrespective of whether I ultimately use the values produced from my model for inferential tasks. Nevertheless, I am open to feedback that would confirm or challenge my interpretation of or decision-making around this topic.

## Appendices {#sec-appen}

### Distribution of Yeo-Johnson-transformed HR scores

![Distribution of Yeo-Johnson-transformed HR Scores (Density & Box Plots)](plots/appendices/yj_dist_plots.png){#fig-yj}

The transformation does rectify the right-skew seen in @fig-hr-dist, if at the expense of introducing a slight left-skew. It also fails to eliminate the bimodality feature.

