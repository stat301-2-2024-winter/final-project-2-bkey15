---
title: "Progress Memo 2"
subtitle: |
  | Final Project 
  | Data Science 2 with R (STAT 301-2)
author: "Brian Key"
date: today

format:
  html:
    toc: true
    embed-resources: true
    theme:
      dark: darkly
    
execute:
  echo: false
  warning: false

from: markdown+emoji 
reference-location: margin
citation-location: margin
---

```{r, echo = FALSE}
#| label: load-pkgs
#| code-fold: false

library(tidyverse)
library(DT)
library(here)
```

::: {.callout-tip icon=false}

## Github Repo Link

[Final Project Github Repo (BDK)](https://github.com/stat301-2-2024-winter/final-project-2-bkey15)

:::

## EDA of Target Variable

In my previous memo, I failed to provide a visualization of the distribution of HR Scores, so I do so here:

![](plots/hr_score_OG_dist_plots.png)

The above shows that HR Scores isn't distributed in a perfectly symmetrical manner, featuring a slight right-skew and a degree of bimodality. Nevertheless, these features seem relatively minor, and most methods available to correct them do not apply given the nature of the variable.^[Namely, square-root, log, and Box-Cox transformations only work for positive variables, but approximately half of HR Scores' values are negative.]

Below, however, is evidence that I have attempted a Yeo-Johnson transformation of HR scores:

![](plots/appendices/yj_dist_plots.png)

The transformation does rectify the right-skew, if at the expense of introducing a slight left-skew. It also fails to eliminate the bimodality feature. In tandem with my present inability to compute the transformed variable's inverse post-model fitting, these considerations prompt me to proceed with HR Scores unaltered, though I welcome feedback as to whether this is the right step to take.

## Commentary on Pre-Recipe Preprocessing

Prior to writing my recipes, I complete the following noteworthy preprocessing steps:

-   Merge the V-Dem, HR Scores, and Political Terror Scale (PTS) datasets. I desired to include variables from the latter in my models, as they were explicitly noted as inputs for the original HR Scores; however, they featured widespread missingness, to the extent that imputing their values proved to be computationally infeasible.
-   For the V-Dem dataset, recode Czechoslovakia's pre-Velvet Divorce (1992) observations as those of the Czech Republic.^[I.e., recode `cowcode == 315` (Czechoslovakia) to `cowcode == 316` (Czech Republic).] Doing so rectifies widespread missingness in this case. I deemed this to be justifiable on two counts:
    1. Fariss assigns the Czech Republic `cowcode` to Czechoslovakia.
    2. V-Dem denominates both Czechoslovakia and the Czech Republic "Czechia"; there is no overlap in naming conventions or otherwise with Slovakia, which doesn't appear in the dataset until 1993.
-   Systematically remove country-years that appear in HR Scores but not in V-Dem---cases shown in @tbl-vdem-miss, below:


```{r, echo = FALSE}
#| label: tbl-vdem-miss
#| tbl-cap: "N Missing from V-Dem by Country"
load(here("data/preprocessed/dattbl_vdem_miss.rda"))
dattbl_vdem_miss
```

-   These observations mainly involve:
    1. Post-Soviet states (e.g., Armenia, Azerbaijan, etc.) and post-WWII states (e.g., West and East Germany).
    2. European microstates (e.g., Liechtenstein and Monaco) and small island states (e.g., Samoa, St. Vincent and the Grenadines, etc.).
-   The former is more tolerable, for they stem from minor discrepancies in coding start/end dates and only pertain to a handful of years. The latter invites more reason for pause; indeed, to delete these observations is effectively to remove the world's smallest countries from the ambit of my project entirely. Nevertheless, it doesn't make sense, in my estimation, to retain them when there exist no data whatsoever---including the most basic of figures, such as population, GDP, etc.---on which reasonably-accurate imputations might be based. I proceed from here, unfortunately having to live with this limitation to my work, though I again welcome feedback as to whether I've made the right choice.
-   Select as predictors the variables I identified in my previous final project as being related in theory to HR Scores, as well as the entirety of V-Dem's high-level and medium-level indices.

Upon considering and completing these steps, I move forward with a preprocessed dataset of 52 variables and 10524 observations. The plot below demonstrates that the preprocessing steps involving HR Scores does not significantly alter the distribution thereof:

![](plots/hr_score_dist_plots.png)

## Baseline Fits

I opted to split my data with proportions of 75-25 training/testing, 90-10 analysis/assessment. The cross-validation split was repeated five times, resulting in a total of 50 folds. @tbl-rmses-base, below, gives the performance of my baseline fits on the cross-validation folds as measured by mean RMSE:

```{r, echo = FALSE}
#| label: tbl-rmses-base
#| tbl-cap: "Comparison of Mean RMSEs by Model"
load(here("data/results/fits_cv/baselines/rmse_kbl_base_fits.rda"))
rmse_kbl_base_fits
```

My null (`null`) and basic baseline (`lm`) models have three recipes each, with each being distinguished by the number of neighbors used for KNN-imputation.

Aside from KNN-imputation, the main preprocessing components shared by each recipe-group are as follows:

-   Transforming country ID (`cowcode`) to a dummy variable.
-   Log-transforming population, GDP, and GDP-per-capita.^[These variables are widely-known as right-skewed, and they are indeed so in the V-Dem dataset. For evidence that I have verified this, see `scripts/appendices/0_appendices.R`.]
-   Normalizing all numeric predictors.

In aggregate, the preprocessing steps results in a training set of 227 variables: 1 outcome, 3 ID variables,^[`year`, `cow_year`, and `country_name`] and 223 predictors---179 of which are country dummies.

Also included as a baseline is a seventh recipe (`akt_lm`), which simply uses my unweighted version of the Political Violence Index (PVI), created in my previous final project, as the sole predictor of HR Score. I do so in order to test the appropriateness of merely substituting it for HR Score.

As we can see, the null models perform well-worse than the basic baseline models; the unweighted PVI baseline splits the difference between the two. There are two important takeaways at this juncture, in my view. First, the basic baseline outperforms the unweighted PVI baseline, meaning we can proceed knowing that there exist better ways to estimate HR Score than to use the unweighted PVI as a simple substitute for HR Score. Second, changes in the number of neighbors used for KNN-imputation seem to have little effect on model performance. This is good insofar as it is no longer a concern of ours; we will proceed with the `step_impute_knn()` default of `neighbors = 5` for all remaining workflows, though we theoretically could have selected a larger number with marginal impact on our findings.

## Tuned Fits

Below is @tbl-rmses-tune, which gives the mean RMSE of the tuned fits on the cross-validation folds:

```{r, echo = FALSE}
#| label: tbl-rmses-tune
#| tbl-cap: "Comparison of Best Mean RMSEs by Model"
load(here("data/results/fits_cv/tuned/rmse_kbl_best_tuned.rda"))
rmse_kbl_best_tuned
```

The ridge, lasso, and elastic-net models use the same recipe as that underpinning the 5-neighbor baselines seen in @tbl-rmses-base. The KNN, random forest, and boosted tree models rely on a similar recipe, the only difference being the inclusion of one-hot encoding for the country-ID dummy variables.^[This means that the total number of predictors leveraged by these models is 224---one more than that of the others.]

The optimal tuning parameters for these models as assessed by mean RMSE were, respectively:

-   KNN: `neighbors = 4`
-   Random Forest: `mtry = 14`, `min_n = 2`
-   Boosted Tree: `mtry = 14`, `min_n = 2`, `learn_rate = 0.631`
-   Elastic Net: `penalty = 0.0000000001`, `mixture = 0.762`
-   Lasso: `penalty = 0.0000000001`
-   Ridge: `penalty = 0.0000000001`

Below is also a plot depicting the confidence intervals for each estimate:

![](plots/ci_plot_best_tuned.png)

@tbl-rmses-tune and the above plot evince that, while the best random-forest model possesses the lowest standard error, it is the best KNN model that has the lowest mean RMSE and confidence interval thereof. As such, we proceed with our final fit by selecting the best KNN model.

## Final Fit

@tbl-metrics-final, below, gives the performance metrics for the final fit applied to the testing set:

```{r}
#| label: tbl-metrics-final
#| tbl-cap: "Performance Metrics from Final Fit"
load(here("data/results/final/final_perform_stats_kbl.rda"))
final_perform_stats_kbl
```

The RMSE is even lower when compared to the cross-validation folds (@tbl-rmses-tune). The MAE is about half has large as the RMSE; and at approximately 0.97, the $R^{2}$ statistic is exceedingly high. (The MAPE is about 49%, but this is a small figure in absolute terms, for the preponderance of HR scores cluster around 0.)

Below is also a scatterplot depicting the relationship between our predictions and actual values in the testing set, illustrating the close fit---and hence high degree of accuracy---of our final model:

![](plots/scatt_plot_final_predicts.png)

are themselves estimates, the mean standard deviation of which is approximately `r round(0.3338768, digits = 2)`.^[For the code computing this figure, see `scripts/appendices/0_appendices.R`.]

## Further Considerations

![](plots/scatt_plot_pvi.png)

