---
title: "Progress Memo 2"
subtitle: |
  | Final Project 
  | Data Science 2 with R (STAT 301-2)
author: "Brian Key"
date: today

format:
  html:
    toc: true
    embed-resources: true
    theme:
      dark: darkly
    
execute:
  echo: false
  warning: false

from: markdown+emoji 
reference-location: margin
citation-location: margin
---

```{r, echo = FALSE}
#| label: load-pkgs
#| code-fold: false

library(tidyverse)
library(here)
```

::: {.callout-tip icon=false}

## Github Repo Link

[Final Project Github Repo (BDK)](https://github.com/stat301-2-2024-winter/final-project-2-bkey15)

:::

## EDA of Target Variable

I my previous memo, I failed to provide a visualization of the distribution of HR Scores, so I do so here:

![](plots/hr_score_dist_plots.png)

The above shows shows that HR Scores isn't distributed in a perfectly symmetrical manner, featuring a slight right-skew and a degree of bimodality. Nevertheless, these features are relatively minor, and methods of correcting them either fail or are not applicable given the nature of the variable.^[Square-root and Box-Cox transformations only work for positive variables; a Yeo-Johnson transformation produces no significant change in the distribution; and a log-transformation results in a strong left-skew. For evidence of attempts at the latter two transformations, see...]

## Baseline Fits

I opted to split my data with proportions of 75-25 training/testing, 90-10 analysis/assessment. The cross-validation split was repeated five times, resulting in a total of 50 folds. @tbl-rmses-base, below, gives the performance of my baseline fits on the cross-validation folds as measured by mean RMSE:

```{r, echo = FALSE}
#| label: tbl-rmses-base
#| tbl-cap: "Comparison of Mean RMSEs by Model"
load(here("data/results/fits_cv/baselines/rmse_kbl_base_fits.rda"))
rmse_kbl_base_fits
```

My null (`null`) and basic baseline (`lm`) models have three recipes each, which each being distinguished by the number of neighbors used for KNN-imputation. Also included as a baseline is a seventh recipe (`akt_lm`), which simply uses my unweighted version of the Political Violence Index (PVI), created in my previous final project, as the sole predictor of HR Score. I do so in order to test the appropriateness of simply substituting it for HR Score.

As we can see, the null models perform well-worse than the basic baseline models; the unweighted PVI baseline splits the difference between the two. There are two important takeaways at this juncture, in my view. First, the basic baseline outperforms the unweighted PVI baseline, meaning we can proceed knowing that there exist better ways to estimate HR Score than to use the unweighted PVI as a simple substitute for HR Score. Second, changes in the number of neighbors used for KNN-imputation seem to have little effect on model performance. This is good insofar as it is no longer a concern of ours; we will proceed with the `step_impute_knn()` default of `neighbors = 5` for all remaining workflows, though we theoretically could have selected a larger number with marginal impact on our findings.

## Tuned Fits

Below is @tbl-rmses-tune, which gives the mean RMSE of the tuned fits on the cross-validation folds:

```{r, echo = FALSE}
#| label: tbl-rmses-tune
#| tbl-cap: "Comparison of Best Mean RMSEs by Model"
load(here("data/results/fits_cv/tuned/rmse_kbl_best_tuned.rda"))
rmse_kbl_best_tuned
```

The ridge, lasso, and elastic-net models used the same recipe as that underpinning the 5-neighbor baselines seen in @tbl-rmses-base. The KNN, random forest, and boosted tree models relied on a similar recipe, the only difference being the inclusion of one-hot encoding for the country-ID dummy variables.

The optimal tuning parameters for these models as assessed by mean RMSE were, respectively:

-   KNN: `neighbors = 4`
-   Random Forest: `mtry = 14`, `min_n = 2`
-   Boosted Tree: `mtry = 14`, `min_n = 2`, `learn_rate = 0.631`
-   Elastic Net: `penalty = 0.0000000001`, `mixture = 0.762`
-   Lasso: `penalty = 0.0000000001`
-   Ridge: `penalty = 0.0000000001`

Below is also a plot depicting the confidence intervals for each estimate:

![](plots/ci_plot_best_tuned.png)

@tbl-rmses-tune and the above plot evince that, while the best random-forest model possesses the lowest standard error, it is the best KNN model that has the lowest mean RMSE and confidence interval thereof. As such, we proceed with our final fit by selecting the best KNN model.

## Final Fit

@tbl-metrics-final, below, gives the performance metrics for the final fit applied to the testing set:

```{r}
#| label: tbl-metrics-final
#| tbl-cap: "Performance Metrics from Final Fit"
load(here("data/results/final/final_perform_stats_kbl.rda"))
final_perform_stats_kbl
```

The RMSE is even lower when compared to the cross-validation folds (@tbl-rmses-tune). The MAE is about half has large as the RMSE; and at approximately 0.97, the $R^{2}$ statistic is exceedingly high. (The MAPE is about 49%, but this is a small figure in absolute terms, for the preponderance of HR scores cluster around 0.)

Below is also a scatterplot depicting the relationship between our predictions and actual values in the testing set, illustrating the close fit---and hence high degree of accuracy---of our final model:

![](plots/scatt_plot_final_predicts.png)

are themselves estimates, the mean standard deviation of which is approximately `r round(0.3338768, digits = 2)`.^[For the code computing this figure, see `scripts/0_appendices.R`.]

## Further Considerations

![](plots/scatt_plot_pvi.png)

